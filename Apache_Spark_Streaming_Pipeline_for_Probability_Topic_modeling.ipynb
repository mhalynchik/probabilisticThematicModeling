{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "l8050x1cdiV2",
        "8dj1JJFKdfdX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "isStreaming = True\n",
        "inputCol = \"abstract\"\n",
        "outputCol = \"probability vectors\"\n",
        "modelType = \"AST\"\n",
        "model = \"AST\" # AST/PLSA/ARTM\n",
        "\n",
        "inputCol = \"abstract\"\n",
        "outputCol = \"probability vectors\""
      ],
      "metadata": {
        "id": "baZlxacFmV2T"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download kaggle token and mount Google drive"
      ],
      "metadata": {
        "id": "FRsvN2SzMneV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qEcKaqyMteL",
        "outputId": "b6f79f31-1da3-4fc2-cc37-3425202f2a56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        " \n",
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!mv ./kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "AXq69LrcMotj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "beeef2f0-98f3-47e5-e7fa-f72ecfc4614b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b73c64a2-8903-4857-8b93-d96195b24608\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b73c64a2-8903-4857-8b93-d96195b24608\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "rm: cannot remove '/root/.kaggle': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install Dependencies"
      ],
      "metadata": {
        "id": "EbeOUNJxA5h8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NFLoRIwzQdSF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pyspark\n",
        "!pip install bigartm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import sys\n",
        "import os\n",
        "# installation of GOT (G eneralization O ver T axonomies) software package\n",
        "!mkdir gotlib\n",
        "if not os.listdir(\"gotlib\"):\n",
        "  !git clone https://github.com/dmitsf/GOT.git gotlib\n",
        "sys.path.append('gotlib')\n",
        "\n",
        "#library name spelled in caps in setup causes errors in Apache Spark\n",
        "!cp /content/drive/MyDrive/setup.py /content/gotlib/setup.py\n",
        "\n",
        "!cd gotlib && python setup.py install"
      ],
      "metadata": {
        "id": "_BnfMtyPKQj-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading taxonomies\n",
        "%%capture\n",
        "!cp /content/drive/MyDrive/arxiv_category_taxonomy.csv arxiv_category_taxonomy.csv \n",
        "\n",
        "!python3 /content/gotlib/got/taxonomies/taxonomy.py arxiv_category_taxonomy.csv "
      ],
      "metadata": {
        "id": "_glnH_UvNFsG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "with open(\"taxonomy_leaves.txt\") as f:\n",
        "    strings = [l.strip() for l in f.readlines()]\n",
        "\n",
        "taxanomy_leaves_df = pd.DataFrame.from_dict(dict(enumerate(strings)), orient='index', columns=[inputCol])"
      ],
      "metadata": {
        "id": "1xC1nIzlNjgh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install kaggle\n",
        "!kaggle datasets download -d Cornell-University/arxiv\n",
        "\n",
        "!mkdir Dataset\n",
        "!cp arxiv.zip Dataset/arxiv.zip\n",
        "!unzip -q Dataset/arxiv.zip -d Dataset\n",
        "!rm Dataset/arxiv.zip"
      ],
      "metadata": {
        "id": "IjR5X_BBDpAC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation of GOT libriary on Apache Spark"
      ],
      "metadata": {
        "id": "WodEY5cR7Qrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf\n",
        "from pyspark.context import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "# add gotlib egg for Spark\n",
        "sc.addFile(path='/content/gotlib/dist/' + os.listdir('/content/gotlib/dist/')[0])"
      ],
      "metadata": {
        "id": "jp8feiLvWcFA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset schema"
      ],
      "metadata": {
        "id": "c3DakXFx7MyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "schema = StructType([\n",
        "StructField(\"id\",StringType(),True), \n",
        "StructField(\"submitter\",StringType(),True), \n",
        "StructField(\"authors\",StringType(),True),\n",
        "StructField(\"title\", StringType(), True),\n",
        "StructField(\"comments\", StringType(), True),\n",
        "StructField(\"journal-ref\", StringType(), True),\n",
        "StructField(\"doi\", StringType(), True),\n",
        "StructField(\"report-no\", StringType(), True),\n",
        "StructField(\"categories\", StringType(), True),\n",
        "StructField(\"abstract\", StringType(), True)])"
      ],
      "metadata": {
        "id": "4HUfxobV641H"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset"
      ],
      "metadata": {
        "id": "e-soPMrX7XUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master('local').appName('Streaming Probability Topic Modeling Pipeline').config('spark.ui.port', '4050').getOrCreate()\n",
        "spark.conf.set('spark.rapids.sql.enabled','true/false')\n",
        "\n",
        "\n",
        "# Arxiv Dataset\n",
        "if isStreaming:\n",
        "  sentenceDataFrame = spark.readStream.format('json').schema(schema).option('header', True).load('Dataset')\n",
        "\n",
        "\n",
        "# Taxonomy \n",
        "taxanomyDataFrame = spark.createDataFrame(taxanomy_leaves_df)"
      ],
      "metadata": {
        "id": "CIFxwxvdbiRW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spark Pipeline Functions"
      ],
      "metadata": {
        "id": "ytWzktGcBULN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Tokenizers"
      ],
      "metadata": {
        "id": "l8050x1cdiV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import keyword_only  ## < 2.0 -> pyspark.ml.util.keyword_only\n",
        "from pyspark.ml import Transformer\n",
        "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasPredictionCol, Param, Params, TypeConverters\n",
        "# Available in PySpark >= 2.3.0 \n",
        "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n",
        "from pyspark.ml.pipeline import Estimator, Model, Pipeline\n",
        "from pyspark.ml.feature import CountVectorizer, StopWordsRemover, RegexTokenizer, CountVectorizerModel\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import artm\n",
        "from got.asts.ast import EASA\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "class ParGenMWordTokenizer(\n",
        "        Transformer, HasInputCol, HasOutputCol,\n",
        "        DefaultParamsReadable, DefaultParamsWritable):\n",
        "\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self, inputCol=None, outputCol=None, lowerize=None):\n",
        "        super(ParGenMWordTokenizer, self).__init__()\n",
        "        self.lowerize =  Param(self, \"lowerize\", \"\")\n",
        "        kwargs = self._input_kwargs\n",
        "        self.setParams(**kwargs)\n",
        "\n",
        "    @keyword_only\n",
        "    def setParams(self, inputCol=None, outputCol=None, lowerize=None):\n",
        "        kwargs = self._input_kwargs\n",
        "        return self._set(**kwargs)\n",
        "\n",
        "    def getLowerize(self):\n",
        "        return self.getOrDefault(self.lowerize)\n",
        "\n",
        "    # Required in Spark >= 3.0\n",
        "    def setInputCol(self, value):\n",
        "        \"\"\"\n",
        "        Sets the value of :py:attr:`inputCol`.\n",
        "        \"\"\"\n",
        "        return self._set(inputCol=value)\n",
        "\n",
        "    # Required in Spark >= 3.0\n",
        "    def setOutputCol(self, value):\n",
        "        \"\"\"\n",
        "        Sets the value of :py:attr:`outputCol`.\n",
        "        \"\"\"\n",
        "        return self._set(outputCol=value)\n",
        "\n",
        "    def _transform(self, dataset):\n",
        "        def clear_text(text):\n",
        "          pat = re.compile(r'[^A-Za-z0-9 \\-\\n\\r.,;!?А-Яа-я]+')\n",
        "          cleared_text = re.sub(pat, ' ', text)\n",
        "\n",
        "          if self.getLowerize():\n",
        "              cleared_text = cleared_text.lower()\n",
        "\n",
        "          tokens = cleared_text.split()\n",
        "          return tokens\n",
        "\n",
        "        t = ArrayType(StringType())\n",
        "        out_col = self.getOutputCol()\n",
        "        in_col = dataset[self.getInputCol()]\n",
        "        return dataset.withColumn(out_col, udf(clear_text, t)(in_col))\n",
        "\n",
        "class StopWordsTokenizer(\n",
        "        Transformer, HasInputCol, HasOutputCol,\n",
        "        DefaultParamsReadable, DefaultParamsWritable):\n",
        "\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self, inputCol=None, outputCol=None, stop_words=None):\n",
        "        super(StopWordsTokenizer, self).__init__()\n",
        "        self.stop_words = Param(self, \"stop_words\", \"\")\n",
        "        kwargs = self._input_kwargs\n",
        "        self.setParams(**kwargs)\n",
        "\n",
        "        self.reTokenizer = RegexTokenizer(pattern='[^A-Za-z\\-А-Яа-я]+', gaps=True, minTokenLength=3)\n",
        "        self.remover = StopWordsRemover(stopWords=stop_words)\n",
        "        self.cv = CountVectorizer(vocabSize=500000, minTF=3, minDF=5, maxDF=5e5)\n",
        "        self.trained = False\n",
        "\n",
        "\n",
        "        # Tokenizer: [\"User defined input column\"] -> [\"words\"]\n",
        "        self.reTokenizer.setOutputCol(\"words\")\n",
        "        \n",
        "        # Remover: [\"words\"] -> [\"tokens\"]\n",
        "        self.remover.setInputCol(\"words\")\n",
        "        self.remover.setOutputCol(\"tokens\")\n",
        "\n",
        "        # CountVectorizer: [\"tokens\"] -> [\"User defined output column\"]\n",
        "        self.cv.setInputCol(\"tokens\")\n",
        "  \n",
        "\n",
        "    @keyword_only\n",
        "    def setParams(self, inputCol=None, outputCol=None, stop_words=None):\n",
        "        kwargs = self._input_kwargs\n",
        "        return self._set(**kwargs)\n",
        "    \n",
        "    def getVocab(self):\n",
        "        return self.vocabulary\n",
        "\n",
        "    # Required in Spark >= 3.0\n",
        "    def setInputCol(self, value):\n",
        "        \"\"\"\n",
        "        Sets the value of :py:attr:`inputCol`.\n",
        "        \"\"\"\n",
        "        return self._set(inputCol=value)\n",
        "\n",
        "    # Required in Spark >= 3.0\n",
        "    def setOutputCol(self, value):\n",
        "        \"\"\"\n",
        "        Sets the value of :py:attr:`outputCol`.\n",
        "        \"\"\"\n",
        "        return self._set(outputCol=value)\n",
        "      \n",
        "    def setTrain(self):\n",
        "      return self._set(train=True)\n",
        "\n",
        "    def _tokenize(self, dataset):\n",
        "        self.reTokenizer.setInputCol(self.getInputCol())\n",
        "        dataset =  self.reTokenizer.transform(dataset)\n",
        "\n",
        "        dataset = self.remover.transform(dataset)\n",
        "        return dataset\n",
        "\n",
        "    def save(self, path):\n",
        "        self.modelcv.save(path)\n",
        "\n",
        "    def load(self, path):\n",
        "        self.modelcv = CountVectorizerModel.load(path)\n",
        "        self.vocabulary = self.modelcv.vocabulary\n",
        "        self.trained = True\n",
        "        \n",
        "\n",
        "    def fit(self, dataset):\n",
        "        dataset = self._tokenize(dataset)\n",
        "        self.cv.setOutputCol(self.getOutputCol())\n",
        "        self.modelcv = self.cv.fit(dataset)\n",
        "        self.vocabulary = self.modelcv.vocabulary\n",
        "        self.trained = True\n",
        "        return self\n",
        "\n",
        "    def _transform(self, dataset):\n",
        "        dataset = self._tokenize(dataset)\n",
        "\n",
        "        if not self.trained:\n",
        "          self.fit(dataset)\n",
        "        \n",
        "        dataset = self.modelcv.transform(dataset)\n",
        "        return dataset"
      ],
      "metadata": {
        "id": "uVD1bfNkQeXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67ce25a9-4b0c-4e1e-c437-9cf0f07cc1fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/gotlib/got/asts/utils.py:33: DeprecationWarning: invalid escape sequence '\\w'\n",
            "  return re.findall(re.compile(\"[\\w']+\", re.U), text)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Estimators"
      ],
      "metadata": {
        "id": "8dj1JJFKdfdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.functions import vector_to_array\n",
        "from pyspark.sql.functions import array\n",
        "\n",
        "class ProbabilityMatrixEstimator(Estimator,\n",
        "        HasInputCol, HasPredictionCol,\n",
        "        DefaultParamsReadable, DefaultParamsWritable):\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self, prepared_string_tokens, inputCol=None, predictionCol=None, \n",
        "                 modelType=\"AST\", vocabulary=None):\n",
        "        super(ProbabilityMatrixEstimator, self).__init__()\n",
        "        self.model = {\"AST\": ASTRelevanceMatrixModel,\n",
        "                      \"PLSA\": PLSAProbabilityMatrixModel,\n",
        "                      \"LDA\": LDAProbabilityMatrixModel,\n",
        "                      \"ARTM\": ARTMProbabilityMatrixModel}\n",
        "        self.modelType =  Param(self, \"modelType\", \"\")\n",
        "        self.prepared_string_tokens =  Param(self, \"prepared_string_tokens\", \"\")\n",
        "        self.vocabulary = Param(self, \"vocabulary\", \"\")\n",
        "        self.prepared_strings = None\n",
        "        kwargs = self._input_kwargs\n",
        "        self.setParams(**kwargs)\n",
        "        \n",
        "        if self.getOrDefault(self.modelType) not in self.model.keys():\n",
        "          raise Exception(\"Wrong model type\")\n",
        "\n",
        "        if self.getOrDefault(self.prepared_string_tokens) is not None:\n",
        "          self.prepared_strings = [' '.join(t) for t in self.getOrDefault(self.prepared_string_tokens)]\n",
        "        self.modelEstimator = None\n",
        "      \n",
        "\n",
        "\n",
        "    # Required in Spark >= 3.0\n",
        "    def setInputCol(self, value):\n",
        "        \"\"\"\n",
        "        Sets the value of :py:attr:`inputCol`.\n",
        "        \"\"\"\n",
        "        return self._set(inputCol=value)\n",
        "\n",
        "    # Required in Spark >= 3.0\n",
        "    def setPredictionCol(self, value):\n",
        "        \"\"\"\n",
        "        Sets the value of :py:attr:`predictionCol`.\n",
        "        \"\"\"\n",
        "        return self._set(predictionCol=value)\n",
        "\n",
        "    def setModelType(self, value):\n",
        "        \"\"\"\n",
        "        Sets the value of :py:attr:`modelType`.\n",
        "        \"\"\"\n",
        "        return self._set(modelType=value)\n",
        "    def getModelEstimator(self):\n",
        "        return self.modelEstimator\n",
        "\n",
        "\n",
        "    @keyword_only\n",
        "    def setParams(self, prepared_string_tokens, inputCol=None, predictionCol=None, modelType=\"AST\", vocabulary=None):\n",
        "        kwargs = self._input_kwargs\n",
        "        return self._set(**kwargs)        \n",
        "        \n",
        "    def _fit(self, dataset):\n",
        "      self.modelEstimator = self.model[self.getOrDefault(self.modelType)](\n",
        "            inputCol = self.getInputCol(),\n",
        "            prepared_strings = self.prepared_strings,\n",
        "            predictionCol = self.getPredictionCol(),\n",
        "            vocabulary=self.getOrDefault(self.vocabulary),\n",
        "            topics_num=15, tokens_num=20, iter_over_document=5, iter_over_collection=20)\n",
        "      return self.modelEstimator\n",
        "\n",
        "        \n",
        "class ProbabilityMatrixModel(Model, HasInputCol, HasPredictionCol,\n",
        "        DefaultParamsReadable, DefaultParamsWritable):\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self, inputCol=None, predictionCol=None,\n",
        "                prepared_strings=None,\n",
        "                vocabulary=None,\n",
        "                topics_num=15, tokens_num=20, iter_over_document=5, iter_over_collection=20):\n",
        "        super(ProbabilityMatrixModel, self).__init__()\n",
        "        self.prepared_strings  =  Param(self, \"prepared_strings\", \"\")\n",
        "        self.vocabulary = Param(self, \"vocabulary\", \"\")\n",
        "\n",
        "        # artm models params(for retrain)\n",
        "        self.topics_num = Param(self, \"topics_num\", \"\")\n",
        "        self.tokens_num = Param(self, \"tokens_num\", \"\")\n",
        "        self.iter_over_document = Param(self, \"iter_over_document\", \"\")\n",
        "        self.iter_over_collection = Param(self, \"iter_over_collection\", \"\")\n",
        "        \n",
        "        kwargs = self._input_kwargs\n",
        "        self.setParams(**kwargs) \n",
        "        self.model = None\n",
        "        \n",
        "\n",
        "    @keyword_only\n",
        "    def setParams(self, inputCol=None, predictionCol=None,\n",
        "                prepared_strings=None,\n",
        "                vocabulary=None,\n",
        "                topics_num=15, tokens_num=20, iter_over_document=5, iter_over_collection=20):\n",
        "        kwargs = self._input_kwargs\n",
        "        return self._set(**kwargs)\n",
        "\n",
        "class ASTRelevanceMatrixModel(ProbabilityMatrixModel):\n",
        "    def _transform(self, dataset):\n",
        "        def make_substrings(tokens, k=4):\n",
        "          for i in range(max(len(tokens) - k + 1, 1)):\n",
        "              yield ' '.join(tokens[i:i + k])\n",
        "\n",
        "        def get_relevance_matrix(cleared_tokens):\n",
        "          ast = EASA(list(make_substrings(cleared_tokens)))\n",
        "          row = [float(ast.score(s)) for s in self.getOrDefault(self.prepared_strings)]\n",
        "          return row\n",
        "\n",
        "        t = ArrayType(DoubleType())\n",
        "        out_col = self.getPredictionCol()\n",
        "        in_col = dataset[self.getInputCol()]\n",
        "        return dataset.withColumn(out_col, udf(get_relevance_matrix, t)(in_col))\n",
        "    \n",
        "    def train(self, dataset):\n",
        "      return self\n",
        "\n",
        "\n",
        "class ARTMModel(ProbabilityMatrixModel):\n",
        "  def _transform(self, dataset):\n",
        "    n_wd = np.apply_along_axis(lambda x: x[0].toArray(), 1, dataset[[self.getInputCol()]].toPandas().to_numpy()).T\n",
        "    bv = artm.BatchVectorizer(data_format='bow_n_wd',\n",
        "                              n_wd=n_wd,\n",
        "                              vocabulary=self.getOrDefault(self.vocabulary))\n",
        "    if self.model is None:\n",
        "      print(\"the ARTM model is not pretrained, training on the transferred dataset\")\n",
        "      self.train(dataset)\n",
        "    res = self.model.transform(bv).T\n",
        "    res['id'] = dataset[['id']].toPandas()\n",
        "    res = spark.createDataFrame(res).select('id', array(['topic_' + str(i) for i in range(1, 15)]).alias(self.getInputCol()))\n",
        "    \n",
        "    return dataset.join(res, dataset.id == res.id, 'left')\n",
        "\n",
        "  def loadModel(self, path):\n",
        "    try:\n",
        "      self.model = artm.load_artm_model(path)\n",
        "    except:\n",
        "      raise Exception(\"Wrong model path or model type\")\n",
        "    return self\n",
        "\n",
        "  def saveModel(self, path):\n",
        "    self.model.dump_artm_model(path)\n",
        "\n",
        "\n",
        "class PLSAProbabilityMatrixModel(ARTMModel):\n",
        "  def train(self, dataset):\n",
        "    n_wd = np.apply_along_axis(lambda x: x[0].toArray(), 1, dataset[[self.getInputCol()]].toPandas().to_numpy()).T\n",
        "    bv = artm.BatchVectorizer(data_format='bow_n_wd',\n",
        "                              n_wd=n_wd,\n",
        "                              vocabulary=self.getOrDefault(self.vocabulary))\n",
        "    \n",
        "    self.model = artm.ARTM(num_topics=self.getOrDefault(self.topics_num), cache_theta=True,\n",
        "                       scores=[artm.PerplexityScore(name='PerplexityScore',\n",
        "                                                    dictionary=bv.dictionary)])\n",
        "    self.model.scores.add(artm.SparsityPhiScore(name='SparsityPhiScore'))\n",
        "    self.model.scores.add(artm.SparsityThetaScore(name='SparsityThetaScore'))\n",
        "    self.model.scores.add(artm.TopicKernelScore(name='TopicKernelScore',\n",
        "                                                probability_mass_threshold=0.3))\n",
        "    self.model.scores.add(artm.TopTokensScore(name='TopTokensScore', \n",
        "                                              num_tokens=self.getOrDefault(self.topics_num)))\n",
        "    self.model.num_document_passes = self.getOrDefault(self.iter_over_document)\n",
        "    \n",
        "    \n",
        "    self.model.initialize(bv.dictionary)\n",
        "    self.model.fit_offline(batch_vectorizer=bv, \n",
        "                           num_collection_passes=self.getOrDefault(self.iter_over_collection))\n",
        "    return self\n",
        "    \n",
        "\n",
        "class LDAProbabilityMatrixModel(ARTMModel):\n",
        "  def train(self, dataset):\n",
        "    n_wd = np.apply_along_axis(lambda x: x[0].toArray(), 1, dataset[[self.getInputCol()]].toPandas().to_numpy()).T\n",
        "    bv = artm.BatchVectorizer(data_format='bow_n_wd',\n",
        "                              n_wd=n_wd,\n",
        "                              vocabulary=self.getOrDefault(self.vocabulary))\n",
        "\n",
        "    self.model = artm.LDA(num_topics=self.getOrDefault(self.topics_num), cache_theta=True)\n",
        "    self.model.num_document_passes = self.getOrDefault(self.iter_over_document)\n",
        "    \n",
        "    \n",
        "    self.model.initialize(bv.dictionary)\n",
        "    self.model.fit_offline(batch_vectorizer=bv, \n",
        "                          num_collection_passes=self.getOrDefault(self.iter_over_collection))\n",
        "    return self\n",
        "\n",
        "class ARTMProbabilityMatrixModel(ARTMModel):\n",
        "  def train(self, dataset):\n",
        "    n_wd = np.apply_along_axis(lambda x: x[0].toArray(), 1, dataset[[self.getInputCol()]].toPandas().to_numpy()).T\n",
        "    bv = artm.BatchVectorizer(data_format='bow_n_wd',\n",
        "                              n_wd=n_wd,\n",
        "                              vocabulary=self.getOrDefault(self.vocabulary))\n",
        "\n",
        "    self.model = artm.ARTM(num_topics=self.getOrDefault(self.topics_num), cache_theta=True,\n",
        "                       scores=[artm.PerplexityScore(name='PerplexityScore',\n",
        "                                                    dictionary=bv.dictionary)],\n",
        "                       regularizers=[artm.SmoothSparseThetaRegularizer(name='SparseTheta',\n",
        "                                                                       tau=-0.15)])\n",
        "    self.model.scores.add(artm.SparsityPhiScore(name='SparsityPhiScore'))\n",
        "    self.model.scores.add(artm.SparsityThetaScore(name='SparsityThetaScore'))\n",
        "    self.model.scores.add(artm.TopicKernelScore(name='TopicKernelScore',\n",
        "                                                      probability_mass_threshold=0.3))\n",
        "    self.model.scores.add(artm.TopTokensScore(name='TopTokensScore', num_tokens=self.getOrDefault(self.tokens_num)))\n",
        "\n",
        "    self.model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='SparsePhi', tau=-0.1))\n",
        "    self.model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='DecorrelatorPhi', tau=1.5e+5))\n",
        "\n",
        "    self.model.num_document_passes = self.getOrDefault(self.iter_over_document)\n",
        "\n",
        "    \n",
        "    self.model.initialize(bv.dictionary)\n",
        "    self.model.fit_offline(batch_vectorizer=bv, num_collection_passes=self.getOrDefault(self.iter_over_collection))\n",
        "    return self\n"
      ],
      "metadata": {
        "id": "rvAGUfgmdeO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a98e1c8-8c54-49ce-d23e-e1daff763559"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pipeline"
      ],
      "metadata": {
        "id": "Zh0rhNueBaZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/count-vectorizer count-vectorizer\n",
        "!cp -r /content/drive/MyDrive/artm_model ARTM\n",
        "!cp -r /content/drive/MyDrive/plsa_model PLSA"
      ],
      "metadata": {
        "id": "MkBm2d0Ai7zR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
      ],
      "metadata": {
        "id": "JIPUl_ZgpiW0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['allow', 'almost', 'also', 'approach', 'asume', 'bad', 'behavior',\n",
        "                  'consider', 'constant', 'control', 'datum', 'density', 'describe',\n",
        "                  'description', 'direction', 'discuss', 'edu', 'effect',\n",
        "                  'effective', 'energy', 'example', 'experimental', 'field', 'find',\n",
        "                  'fine', 'first', 'form', 'from', 'give', 'high', 'investigate',\n",
        "                  'know', 'known', 'large', 'lead', 'let', 'long', 'low', 'make', 'model',\n",
        "                  'new', 'non', 'observe', 'obtain', 'paper', 'parameter',\n",
        "                  'particular', 'point', 'positive', 'present', 'problem',\n",
        "                  'property', 'propose', 'result', 'sample', 'search', 'show',\n",
        "                  'small', 'state', 'study', 'subject', 'suggest', 'suppose',\n",
        "                  'system', 'theory', 'time', 'use', 'well', 'word', 'work', ' ', ''])"
      ],
      "metadata": {
        "id": "DHx_7TjsiHN7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat_ws, col\n",
        "\n",
        "tokenizer = {\"AST\": ParGenMWordTokenizer(\n",
        "    inputCol=inputCol, outputCol=\"tokens\",  \n",
        "    lowerize=True), \n",
        "            \"ARTM\": StopWordsTokenizer(\n",
        "    inputCol=inputCol, outputCol=\"vectors\", \n",
        "    stop_words=stop_words)}\n",
        "\n",
        "#load pretrained tokenizer\n",
        "tokenizer[\"ARTM\"].load(\"count-vectorizer\")\n",
        "\n",
        "# Tokenize Taxonomy for AST\n",
        "taxanomyDataFrame = tokenizer[\"AST\"].transform(taxanomyDataFrame)\n",
        "string_tokens = taxanomyDataFrame.toPandas().tokens\n",
        "\n",
        "estimator = ProbabilityMatrixEstimator(inputCol=\"tokens\", \n",
        "                                        predictionCol=outputCol, \n",
        "                                        modelType=model,\n",
        "                                        prepared_string_tokens=string_tokens,\n",
        "                                        vocabulary=tokenizer[\"ARTM\"].getVocab())\n",
        "\n",
        "if modelType == \"ARTM\":\n",
        "  estimator.loadModel(model)\n",
        "  estimator.setInputCol(\"vectors\")\n",
        "\n",
        "model_pipeline = Pipeline(stages=[tokenizer[modelType], estimator])\n",
        "\n",
        "res = model_pipeline.fit(sentenceDataFrame).transform(sentenceDataFrame)\n",
        "\n",
        "df = res.select(\n",
        "    res.id,\n",
        "    concat_ws(\", \", col(outputCol).cast(\"array<string>\")).alias(outputCol)\n",
        ")"
      ],
      "metadata": {
        "id": "zRajLd_qhfq0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = (df.writeStream\n",
        ".format('json')\n",
        ".queryName('ParGenM')\n",
        ".option('checkpointLocation', 'checkpoint')\n",
        ".option('path', 'results')\n",
        ".outputMode('append')\n",
        ".start()\n",
        ".awaitTermination() \n",
        ")"
      ],
      "metadata": {
        "id": "E0g_vAPxxIkI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "2769320c-a7b7-4d07-94f8-36b0e6f97d24"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-dbaf7e95a35d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/query.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}